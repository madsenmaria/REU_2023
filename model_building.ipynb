{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 12:21:08.880733: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=12+3\n",
    "list(range(3, x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put data in a data folder\n",
    "df_olr = xr.load_dataset('olr_data.nc') \n",
    "df_wg = xr.load_dataset('wind_geopot_data.nc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking out 1982/83 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out 1982-5 -> 1983-5\n",
    "cut_time_slice1 = slice('1979', '1982-5')\n",
    "cut_time_slice2 = slice('1983-11', '2019')\n",
    "\n",
    "df_olr = xr.merge([df_olr.sel(time=cut_time_slice1), df_olr.sel(time=cut_time_slice2)])\n",
    "df_wg = xr.merge([df_wg.sel(time=cut_time_slice1), df_wg.sel(time=cut_time_slice2)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking out the Anomolies for OLR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate olr anomolies\n",
    "\n",
    "#take the 21-day running means (for olr by mean)\n",
    "olr_short = df_olr.olr.values\n",
    "olr_byyear = np.reshape(olr_short, (212, 39, 13, 144))\n",
    "\n",
    "season_meanolr = np.zeros((212, 13, 144))\n",
    "\n",
    "for i in range(1,212):\n",
    "    season_meanolr[i, :,:] = np.mean(np.squeeze(olr_byyear[i,:,:,:]))\n",
    "\n",
    "season_meanolr\n",
    "\n",
    "olr_day = np.roll(season_meanolr, (21,1))\n",
    "\n",
    "\n",
    "olr_season = np.zeros((212, 39, 13, 144))\n",
    "\n",
    "#subtract out the 21-day running mean to get anomalies\n",
    "for i in range(1,39):\n",
    "    for j in range(1,212):\n",
    "        olr_season[j, i, :, :] = np.squeeze(olr_byyear[j,i,:,:]) - np.squeeze(olr_day[j,:,:])\n",
    "olr_season\n",
    "\n",
    "\n",
    "#take 5-day running mean to smooth out the data\n",
    "olr = olr_season\n",
    "olr = np.zeros((212, 39, 13, 144))\n",
    "\n",
    "for i in range(1,39):\n",
    "    olr[:, i, :, :] = np.roll(np.squeeze(olr_season[:, i, :, :]), (5,1))\n",
    "\n",
    "\n",
    "df_olr.olr.values = np.reshape(olr, (8268, 13, 144))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Normalized Graphs of the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing z500 data\n",
    "z500_means = np.mean(df_wg.z500.values)\n",
    "z500_std = np.std(df_wg.z500.values)\n",
    "df_wg.z500.values = (df_wg.z500.values - z500_means)/z500_std\n",
    "\n",
    "#normalizing u250 data\n",
    "u250_means = np.mean(df_wg.u250.values)\n",
    "u250_std = np.std(df_wg.u250.values)\n",
    "df_wg.u250.values = (df_wg.u250.values - u250_means)/u250_std\n",
    "\n",
    "#normalizing olr data\n",
    "olr_means = np.mean(df_olr.olr.values)\n",
    "olr_std = np.std(df_olr.olr.values)\n",
    "df_olr.olr.values = (df_olr.olr.values - olr_means)/olr_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of all the dates being used\n",
    "label_list = list(np.datetime_as_string(df_olr.time.values))\n",
    "for i, val in enumerate(label_list):\n",
    "    label_list[i] = val.split('T')[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating PPH Data by Location and Type\n",
    "This is where I made the daily means for pph values at each location. It took forever to run so I commented it out and downloaded it as a csv file called ('pph_loc_mean.csv'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>west_tor</th>\n",
       "      <th>west_wind</th>\n",
       "      <th>west_hail</th>\n",
       "      <th>central_tor</th>\n",
       "      <th>central_wind</th>\n",
       "      <th>central_hail</th>\n",
       "      <th>east_tor</th>\n",
       "      <th>east_wind</th>\n",
       "      <th>east_hail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979-11-01</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-11-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-11-03</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-11-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-11-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-27</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.274401</td>\n",
       "      <td>3.712219</td>\n",
       "      <td>4.844967</td>\n",
       "      <td>5.126097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-28</th>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>1.073510</td>\n",
       "      <td>3.527014</td>\n",
       "      <td>5.957370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587768</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>2.231628</td>\n",
       "      <td>5.683658</td>\n",
       "      <td>2.266128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-30</th>\n",
       "      <td>0.092477</td>\n",
       "      <td>0.231149</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>1.137013</td>\n",
       "      <td>1.142408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.121624</td>\n",
       "      <td>0.128138</td>\n",
       "      <td>1.318937</td>\n",
       "      <td>1.646136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>1.400037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8268 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            west_tor  west_wind  west_hail  central_tor  central_wind  \\\n",
       "date                                                                    \n",
       "1979-11-01  0.000000   0.000000   0.000000     0.000000      0.000000   \n",
       "1979-11-02  0.000000   0.000000   0.000000     0.000000      0.000000   \n",
       "1979-11-03  0.000000   0.000000   0.000000     0.000000      0.000000   \n",
       "1979-11-04  0.000000   0.000000   0.000000     0.000000      0.000000   \n",
       "1979-11-05  0.000000   0.000000   0.000000     0.000000      0.000000   \n",
       "...              ...        ...        ...          ...           ...   \n",
       "2019-05-27  0.200550   0.038387   0.274401     3.712219      4.844967   \n",
       "2019-05-28  0.012979   0.002045   0.064389     1.073510      3.527014   \n",
       "2019-05-29  0.000000   0.000008   0.000890     2.231628      5.683658   \n",
       "2019-05-30  0.092477   0.231149   0.041616     0.003786      1.137013   \n",
       "2019-05-31  0.013914   0.013914   0.121624     0.128138      1.318937   \n",
       "\n",
       "            central_hail  east_tor  east_wind  east_hail  \n",
       "date                                                      \n",
       "1979-11-01      0.000000       0.0   0.000000   0.000000  \n",
       "1979-11-02      0.000000       0.0   0.000000   0.000000  \n",
       "1979-11-03      0.000000       0.0   0.000000   0.000000  \n",
       "1979-11-04      0.000000       0.0   0.000000   0.000000  \n",
       "1979-11-05      0.000000       0.0   0.000000   0.000000  \n",
       "...                  ...       ...        ...        ...  \n",
       "2019-05-27      5.126097       0.0   0.000000   0.000000  \n",
       "2019-05-28      5.957370       0.0   0.587768   0.000000  \n",
       "2019-05-29      2.266128       0.0   0.000000   0.000000  \n",
       "2019-05-30      1.142408       0.0   0.000000   0.000000  \n",
       "2019-05-31      1.646136       0.0   0.004869   1.400037  \n",
       "\n",
       "[8268 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pph_data = pd.read_csv('pph_loc_mean.csv').rename(columns={'Unnamed: 0': 'date'}).set_index('date')\n",
    "pph_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data to output time-series-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating windows and horizons\n",
    "HORIZON = 7 # PPH from 7 days afterwords\n",
    "WINDOW = 30 # assumes sliding in days\n",
    "\n",
    "\n",
    "# creates a expanding window from given label -- WORKS!\n",
    "def get_window_expanding(label=\"\", label_list=label_list):\n",
    "    try:\n",
    "        # take the season\n",
    "        # season_index = label_list.index(label)\n",
    "\n",
    "        # grab all the days previous to one that you have\n",
    "        window = [0]*(len(label_list)-7)\n",
    "        for i in range(len(window)):\n",
    "            window[i] = label_list[len(label_list)-i-8]\n",
    "        # return window and label\n",
    "        return window, label\n",
    "        \n",
    "    \n",
    "    #printing error when value is not in seasons list\n",
    "    except:\n",
    "        return \"Label not found in list\"\n",
    "    \n",
    "\n",
    "# creates a sliding window from given label -- WORKS!\n",
    "def get_window_sliding (label=\"\", window_num=WINDOW):\n",
    "    try: \n",
    "        #ensure not grabbing from diff season\n",
    "        day_index = label_list.index(label)\n",
    "        m = int(label.split('-')[1])\n",
    "        d = int(label.split('-')[2])\n",
    "\n",
    "        #for values that are almost outside the season, use and expanding window \n",
    "        if (m ==11 ) or (m ==12 and d < 7):\n",
    "            y = label.split('-')[0]\n",
    "            stopper = y+'-11-01' #end of the season\n",
    "            stop_index = label_list.index(stopper)\n",
    "\n",
    "            #creates a new list to grab data from\n",
    "            label_list_copy = label_list.copy()[stop_index:day_index+1]\n",
    "\n",
    "            return (get_window_expanding(label=label, label_list=label_list_copy))\n",
    "\n",
    "        else:\n",
    "            window = [0]*(window_num)\n",
    "\n",
    "            for i in range(len(window)):\n",
    "                window[i] = label_list[day_index-i-7] #TODO: 7 days between or pph_day - last_day = 7\n",
    "            return window, label  \n",
    "    \n",
    "    except:\n",
    "        return \"Label not found in list\"\n",
    "    \n",
    "\n",
    "# returns wind and geopotential dataframe for prev 30 days (minus 7d), olr dataframe for prev 30 days(minus 7d), \n",
    "# and pph data for the given date\n",
    "def get_data(date):\n",
    "    dates = get_window_sliding(date)\n",
    "    past_dates = dates[0]\n",
    "    target_data = pph_data.loc[date]\n",
    "    # print(len(past_dates))\n",
    "    if len(past_dates) == 0:\n",
    "        return [], [], target_data\n",
    "    slicer = slice(past_dates[-1], past_dates[0])\n",
    "\n",
    "    return df_wg.sel(time=slicer), df_olr.sel(time=slicer), target_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the Data into Train, Test, and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = label_list[label_list.index('1990-11-01'):label_list.index('2000-05-31')]\n",
    "# train_labels = label_list[label_list.index('1983-11-01'):label_list.index('2016-05-31')]\n",
    "# TODO: run above when using schooner\n",
    "# should be what is above but you don't have enough space rn\n",
    "\n",
    "test_labels = label_list[label_list.index('2016-11-01'):label_list.index('2019-05-31')]\n",
    "val_labels = label_list[label_list.index('1979-11-01'):label_list.index('1982-05-31')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = label_list[label_list.index('1990-11-01'):label_list.index('2000-05-31')]\n",
    "test_labels = label_list[label_list.index('2016-11-01'):label_list.index('2019-05-31')]\n",
    "val_labels = label_list[label_list.index('2013-11-01'):label_list.index('2016-05-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_era5_data = np.zeros((2119, 30, 51, 241, 2))\n",
    "\n",
    "\n",
    "for i, val in enumerate(train_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].u250.values) == 0:\n",
    "            train_era5_data[i,:,:, :,0] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].u250.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            train_era5_data[ i,:,:, :,0] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].u250.values\n",
    "            train_era5_data[i,:,:, :,0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        train_era5_data[i,:,:, :,0] = np.full((30, 51, 241), 0)\n",
    "\n",
    "\n",
    "\n",
    "for i, val in enumerate(train_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].z500.values) == 0:\n",
    "            train_era5_data[i,:,:, :,1] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].z500.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            train_era5_data[i,:,:, :,1] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].z500.values\n",
    "            train_era5_data[i,:,:, :,1] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        train_era5_data[i,:,:, :,1] = np.full((30, 51, 241), 0)\n",
    "\n",
    "\n",
    "\n",
    "train_olr_data = np.zeros(( 2119, 30, 13, 144, 1))\n",
    "for i, val in enumerate(train_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[1].olr.values) == 0:\n",
    "            train_olr_data[i,:,:, :,0] = np.full((30, 13, 144), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30,  13, 144))\n",
    "            n = get_data(val)[1].olr.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 13, 144), 0)\n",
    "            train_olr_data[i,:,:, :,0] = r\n",
    "        else:\n",
    "            d = get_data(val)[1].olr.values\n",
    "            train_olr_data[i,:,:, :,0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        train_olr_data[i,:,:, :,0] = np.full((30, 13, 144), 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.27907324, 0.08914221, ..., 0.        , 0.03325079,\n",
       "        0.        ],\n",
       "       [0.        , 0.08857632, 0.10432676, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 1.91484814, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pph_data = np.zeros((2119, 9))\n",
    "for i, val in enumerate(train_labels):\n",
    "    train_pph_data[i,:] = np.array(list(get_data(val)[2].to_dict().values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_era5_data = np.zeros((635, 30, 51, 241, 2))\n",
    "for i, val in enumerate(test_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].u250.values) == 0:\n",
    "            test_era5_data[i,:,:,:, 0] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].u250.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            test_era5_data[i,:,:,:, 0] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].u250.values\n",
    "            test_era5_data[i,:,:,:, 0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        test_era5_data[i,:,:,:, 0] = np.full((30, 51, 241), 0)\n",
    "\n",
    "\n",
    "\n",
    "for i, val in enumerate(test_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].z500.values) == 0:\n",
    "            test_era5_data[i,:,:,:, 1] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].z500.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            test_era5_data[i,:,:,:, 1] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].z500.values\n",
    "            test_era5_data[i,:,:,:, 1] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        test_era5_data[i,:,:,:, 1] = np.full((30, 51, 241), 0)\n",
    "\n",
    "test_olr_data = np.zeros((635, 30, 13, 144, 1))\n",
    "for i, val in enumerate(test_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[1].olr.values) == 0:\n",
    "            test_olr_data[i,:,:,:,0] = np.full((30, 13, 144), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30,  13, 144))\n",
    "            n = get_data(val)[1].olr.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 13, 144), 0)\n",
    "            test_olr_data[i,:,:,:,0] = r\n",
    "        else:\n",
    "            d = get_data(val)[1].olr.values\n",
    "            test_olr_data[i,:,:,:,0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        test_olr_data[i,:,:,:,0] = np.full((30, 13, 144), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.29205303e-04, 9.15009754e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.29787771e-02, 2.04527881e-03, 6.43886468e-02, ...,\n",
       "        0.00000000e+00, 5.87768439e-01, 0.00000000e+00],\n",
       "       [0.00000000e+00, 8.32211831e-06, 8.89534259e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.24768088e-02, 2.31149184e-01, 4.16156313e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pph_data = np.zeros((635, 9))\n",
    "for i, val in enumerate(test_labels):\n",
    "    test_pph_data[i,:] = np.array(list(get_data(val)[2].to_dict().values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_era5_data = np.zeros((635, 30, 51, 241, 2))\n",
    "\n",
    "# print('here')\n",
    "for i, val in enumerate(val_labels):\n",
    "    # print(val)\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].u250.values) == 0:\n",
    "            val_era5_data[i,:,:,:,0] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].u250.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            val_era5_data[i,:,:,:,0] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].u250.values\n",
    "            val_era5_data[i,:,:,:,0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        val_era5_data[i,:,:,:,0] = np.full((30, 51, 241), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, val in enumerate(val_labels):\n",
    "    # print(val)\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[0].z500.values) == 0:\n",
    "            val_era5_data[i,:,:,:,1] = np.full((30, 51, 241), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30, 51, 241))\n",
    "            n = get_data(val)[0].z500.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 51, 241), 0)\n",
    "            val_era5_data[i,:,:,:,1] = r\n",
    "        else:\n",
    "            d = get_data(val)[0].z500.values\n",
    "            val_era5_data[i,:,:,:,1] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        val_era5_data[i,:,:,:,1] = np.full((30, 51, 241), 0)\n",
    "\n",
    "\n",
    "\n",
    "val_olr_data = np.zeros((635, 30, 13, 144, 1))\n",
    "for i, val in enumerate(val_labels):\n",
    "    try:\n",
    "        day_index = label_list.index(val)\n",
    "        m = int(val.split('-')[1])\n",
    "        d = int(val.split('-')[2])\n",
    "        if len(get_data(val)[1].olr.values) == 0:\n",
    "            val_olr_data[i,:,:,:,0] = np.full((30, 13, 144), 0)\n",
    "        \n",
    "        elif(m ==11) or (m ==12 and d < 7):\n",
    "            r = np.zeros((30,  13, 144))\n",
    "            n = get_data(val)[1].olr.values\n",
    "            r[0:len(n), :, :] = n\n",
    "            r[len(n):, :, :] = np.full(((30-len(n)), 13, 144), 0)\n",
    "            val_olr_data[i,:,:,:,0] = r\n",
    "        else:\n",
    "            d = get_data(val)[1].olr.values\n",
    "            val_olr_data[i,:,:,:,0] = d\n",
    "        # print(val)\n",
    "    except Exception as e:\n",
    "        val_olr_data[i,:,:,:,0] = np.full((30, 13, 144), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 4.17033176e-03, 1.71284185e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.83343854e-02, 0.00000000e+00, 1.22424421e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.27130537e-04, 0.00000000e+00, 1.59175823e-03, ...,\n",
       "        0.00000000e+00, 5.87768439e-01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pph_data = np.zeros((635, 9))\n",
    "for i, val in enumerate(val_labels):\n",
    "    val_pph_data[i,:] = np.array(list(get_data(val)[2].to_dict().values()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input(shape=(#steps,#features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a conv2d and max pool sequence \n",
    "# rounds: how many conv2d-max pool sequences wanted\n",
    "# n_c2d_filters: filters\n",
    "# num filters/rounds = how many conv2d layers will be in each round/sequence\n",
    "\n",
    "def conv_2d_max_pool_seq (input_x, rounds, n_c2d_filters, activation, kernel_size=3):\n",
    "  if len(n_c2d_filters)%rounds != 0:\n",
    "    return \"Incorrect number of filters\"\n",
    "  else:\n",
    "    i = 0\n",
    "    x = input_x\n",
    "    d = int(len(n_c2d_filters)/rounds)\n",
    "    while i < rounds:\n",
    "      for n in range(d):\n",
    "        x = layers.Conv3D(filters=n_c2d_filters[int(n+d*i)], kernel_size= kernel_size, activation=activation, padding='same')(x)\n",
    "      # print(d)\n",
    "      x = layers.MaxPooling3D((2,2,2))(x)\n",
    "      i = i+1\n",
    "    x = layers.Flatten()(x)\n",
    "    return(x)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make her better for hyperparameter tuning\n",
    "\n",
    "input_wg = keras.Input(shape = ((30, 51, 241, 2)), name =\"era5 input\")\n",
    "input_olr = keras.Input(shape = ((30, 13, 144, 1)), name =\"olr input\")\n",
    "\n",
    "#dense output = 3 -> east, central, west \n",
    "\n",
    "#This model puts everything together; TODO: re-order parameters\n",
    "def model_maker(input_wg, input_olr, rounds, n_c2d_filters, hidden_nodes, activation, name, dropout_rate, kernel_reg, kernel_size=3, learning_rate = 0.01,): \n",
    "    x = layers.concatenate([conv_2d_max_pool_seq(input_wg, rounds, n_c2d_filters, activation, kernel_size), \n",
    "                            conv_2d_max_pool_seq(input_olr, rounds, n_c2d_filters, activation, kernel_size)])\n",
    "    \n",
    "    #need to also concatenate \n",
    "\n",
    "    # for h in hidden_nodes:\n",
    "    #     # x = layers.Dropout(dropout_rate)(x)\n",
    "    #     x = layers.Dense(h, activation=activation)(x)\n",
    "\n",
    "    outputs = layers.Dense(9)(x) #ridge regression for regulizer\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs = [input_wg, input_olr], \n",
    "        outputs = outputs, \n",
    "        name=name)\n",
    "    \n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate = learning_rate,\n",
    "                                   amsgrad = False)\n",
    "    \n",
    "    # Bind the model to the optimizer\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_maker(input_wg, input_olr, rounds=1, n_c2d_filters=[9], hidden_nodes=[9], \n",
    "            dropout_rate=0.001, activation='relu', kernel_reg='l1', name='first_iter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m early_stopping_cb \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                                   restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                                                       min_delta\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      5\u001b[0m h \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39m[train_era5_data, train_olr_data], y\u001b[39m=\u001b[39m[train_pph_data], epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m                         validation_data\u001b[39m=\u001b[39m([val_era5_data, val_olr_data], [val_pph_data]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True,\n",
    "                                                      min_delta=0.01)\n",
    "\n",
    "h = model.fit(x=[train_era5_data, train_olr_data], y=[train_pph_data], epochs=10, verbose=1,\n",
    "                        validation_data=([val_era5_data, val_olr_data], [val_pph_data]), \n",
    "                        steps_per_epoch = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
